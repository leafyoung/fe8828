# Lecture 12: Building Predictive Model

## Machine Learning

```{r echo = FALSE, out.width = "80%"}
knitr::include_graphics("imgs/2019/ml_algo.png")
```

## Statistics and Machine Learning

```{r echo = FALSE, out.width = "40%"}
knitr::include_graphics("imgs/2019/comic-stat-vs-ml.jpg")
```

## Statistics and Machine Learning

- Statistics is a age-old year subject, with many developed theory.
- Machine learning is an algorithm that can learn from data without relying on rules-based.
- ML uses many statistical theories in application.
- ML emphasizes optimization and performance (accuracy) over inference (conclusion based on reasons and evidence) which is what statistics is concerned about.

    | ML professional: "The model is 85% accurate in predicting Y, given a, b and c."
    | Statistician: “The model is 85% accurate in predicting Y, given a, b and c; and I am 90% certain that you will obtain the same result.” 

## Supervised v.s. Unsupervised

- Supervised learning: It is based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples
- Unsupervised learning: It is a type of self-organized learning that helps find previously unknown patterns in data set without pre-existing labels. It is also known as self-organization and allows modeling probability densities of given inputs.
- Reinforcement learning: ...

## Machine Learning

- Regression
- Classification
- Ensemble
  - hetreogenous ensembles
  - homogenous ensembles
  - metalmodelling
- Neutral network
  - Deep learning: multiple hidden layers.

## Model Error

  model error = variance + bias + noise
  
  - variance-bias trade-off: increase variance and more bias

## Regression and Classification

Regression and classification are two main categories of machine learning algorithms under supervised learning.

```{r echo = FALSE, out.width = "60%"}
knitr::include_graphics("imgs/2019/class_reg.png")
```

## Regression

We can use many linear regression methods.

```{r echo=TRUE}
p1 <- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Petal.Width ~ Sepal.Length")

p2 <- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Petal.Width ~ Petal.Length")

p3 <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Sepal.Width ~ Sepal.Length")

p4 <- ggplot(iris, aes(x = Petal.Length, y = Sepal.Width, color = Species)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Sepal.Width ~ Petal.Length")
```

## Regression

```{r echo=FALSE, out.width="100%"}
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

## Classification

Decision Tree

```{r}
library(rpart)
iris_rp <- rpart(Species ~ ., iris)
```

```{r eval = FALSE, include=TRUE}
# try below statment
summary(iris_rp)
rattle::ggVarImp(iris_rp)
rattle::fancyRpartPlot(iris_rp)
```

```{r echo=FALSE, out.width="70%"}
rattle::fancyRpartPlot(iris_rp)
```

```{r echo=TRUE}
predictions <- predict(iris_rp, iris, type = "class")
which(iris$Species != predictions)
# caret::confusionMatrix(predictions, iris$Species)
```

## Ensemble
- Bagging: reducing bias and keep variance in control
- AdaBoost: wrong result will get more weight
- GradientBoost: reduce on residue
- Random forest: random selection of features and component tree to be flexible, no pruning.
    - "bumping" is to use the most effective tree "dtree" will use it.
- Stacking/Subsemble/SuperLearner


## Confusion Matrix
- Shows how model performs

      Actual Target |          0        |  1  |
      Model Output  |                   |     |
                 0  |         90        | 10  |
                 1  |         10        | 90  |


## Machine Learning workflow

1. Setting
2. Exploratory Data Analysis
3. Feature Engineering
4. Data Preparation
5. Modelling
6. Conclusion

## Machine Learning

- Data preparation:

Split into different groups. For simple data, we may split using 75/25 or 80/20. For complex data, we need to ensure selected 75 has coverage for different kinds, to avoid bias. For example, we are to predict a infrequent event (occuring 20%), shall our selected sample contain 20% or 50% or 75%?

- Modeling:

Choose a few models and tune the hyperparametes. Tuning of hyperparameters is model-specific. You shall learn it in-depth with each model.

Measure the performacne and optimize it.
Confusion matrix, AUC/ROC, model-specific output...

## `Caret` Package

Caret is short for _C_lassification _A_nd _RE_gression _T_raining.

```{r echo = FALSE, out.width = "50%"}
knitr::include_graphics("imgs/2019/Caret-package-in-R.png")
```

Main author is Max Kuhn. He has a book "Applied Predictive Modeling", By Max Kuhn and Kjell Johnson.

Max is now in RStudio, working on _parsnip_ in tidymodel.

```{r echo = FALSE, out.width = "30%"}
knitr::include_graphics("imgs/2019/applied-predictive-modeling-book-cover.png")
```

## `Caret` Package

Links to 200 over (238 as of this version)
https://topepo.github.io/caret/available-models.html

```{r echo = FALSE}
library(caret)
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
```

## Project Iris

Use the petal/sepal width/length to determine which species it is.

```{r echo=TRUE, eval = FALSE}
library(caret)

set.seed(123)
trainIndex <- createDataPartition(iris$Species, p = .8,
 list = FALSE,
 times = 1)

train <- iris[ trainIndex,]
test  <- iris[-trainIndex,]

train_x <- select(train, -Species)
train_y <- train$Species

test_x <- select(test, -Species)
test_y <- test$Species

# Cross validation
fitControl <- trainControl(
 method = "repeatedcv",
 number = 10,
 repeats = 5)

# first run may need to do package installation. Caret
# install.packages("e1071")
# recursive partition = decision tree
dt_fit <- train(Species ~ ., data = train,
 method = "rpart",
 trControl = fitControl,
 preProcess=c("center", "scale"))

dt_fit
plot(dt_fit)

predictions <- predict(dt_fit, test)
confusionMatrix(predictions, test$Species)

# random Forests
rf_fit <- train(Species ~ .,
                data = train,
                method = "ranger")

rf_fit
plot(rf_fit)

predictions <- predict(rf_fit, test)
confusionMatrix(predictions, test$Species)
which(y_test != predictions)
y_test[which(y_test != predictions)]
```


```{r eval = FALSE, include = FALSE}
# install.packages("xgboost")
library(xgboost)

X_train <- xgb.DMatrix(as.matrix(train %>% select(-Species)))
y_train <- train$Species
X_test <- xgb.DMatrix(as.matrix(test %>% select(-Species)))
y_test <- test$Species

# Specify cross-validation method and number of folds. Also enable parallel computation

xgb_trcontrol = trainControl(
  method = "cv",
  number = 5,  
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

# This is the grid space to search for the best hyperparameters

xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
                       max_depth = c(10, 15, 20, 25),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5),
                       ## The values below are default values in the sklearn-api. 
                       eta = 0.1,
                       gamma=0,
                       min_child_weight = 1,
                       subsample = 1
                      )

# Finally, train your model

set.seed(0) 
xgb_model <- train(
  X_train, y_train,  
  trControl = xgb_trcontrol,
  tuneGrid = xgbGrid,
  method = "xgbTree"
)

# Best values for hyperparameters
xgb_model$bestTune

predictions <- predict(xgb_model, X_test)
confusionMatrix(predictions, y_test)
which(y_test != predictions)
y_test[which(y_test != predictions)]

```

## Project Bank

We previous whether our telemarketing is successful.

The output is binary - classification for binary output is very common.

For model simplicity, we use logistic regression. For logistic regression, we need to use one-hot encoding.

One-hot Encoding is implemented with dummy variables in R.

If one column contains more-than-one value, e.g. "admin", "engineer", "manager", we replace it with 2 = 3 - 1 columns

      | job      |        | job_admin | job_engineer |
      | admin    |   ==>  |     1     |       0      |
      | engineer |        |     0     |       1      |
      | manager  |        |     0     |       0      |


```{r eval=FALSE, include=FALSE}
# Manual dummy variable creation, skip.
deco_prefix <- function(df_in, prefix) {
  nn_nn <- which(colnames(df_in) == "nn")
  nn_max <- length(colnames(df_in))
  names_df <- colnames(df_in)
  colnames(df_in) <- c(names_df[1:nn_nn], paste0(prefix, names_df[(nn_nn+1):nn_max]))
  df_in %>% select(-nn) %>% mutate(nn = 1:n())
}

bank_new <- bank_fit %>%
  mutate(nn = 1:n())
  spread(job, job, fill = "0") %>% 
  deco_prefix("job.") %>%
  spread(marital, marital, fill = "0") %>%
  deco_prefix("marital.") %>%
  mutate_at(vars(starts_with("job.")), function(x) { ifelse(x == "0", 0, 1) }) %>%
  mutate_at(vars(starts_with("marital.")), function(x) { ifelse(x == "0", 0, 1) }) %>%
  mutate(loan = ifelse(loan == "yes", 1, 0),
         default = ifelse(default == "yes", 1, 0),
         housing = ifelse(housing == "yes", 1, 0),
         poutcome = ifelse(poutcome == "yes", 1, 0),
         y = ifelse(y == "yes", "y", "n")) %>%
  select(-nn)
# bank_new <- bank_new %>% mutate(y = factor(y))
```

## Project Bank - Load

```{r echo=TRUE, eval = FALSE}
bank <- read.csv("https://goo.gl/PBQnBt", sep = ";")
# We only work on following fields.
bank_fit <- bank %>% select(y,
                loan,
                default,
                housing,
                poutcome,
                job,
                marital) %>%
  mutate_if(is.factor, as.character) %>%
  mutate(y = ifelse(y == "yes", "y", "n"))

str(bank_fit)
```

## Project Bank - Dummy Variables

```{r echo=TRUE, eval = FALSE}
# create dummy variables
dummies <- dummyVars("y ~ loan + default + housing + poutcome + job + marital",
                     data = bank_fit, fullRank = TRUE)
# generate data frame of dummy variables
bank_new <- tibble(predict(dummies, newdata = bank_fit))
# add back y variable to data
bank_new <- bind_cols(bank_fit["y"], bank_new) %>% mutate(y = factor(y))

summary(bank_new)
```

## Project Bank - Train/Test Data

```{r echo=TRUE, eval = FALSE}
# library(caret)
set.seed(1234)
trainIndex <- createDataPartition(bank_new$y, p = .8,
 list = FALSE,
 times = 1)

bank_train <- bank_new[ trainIndex,]
bank_test  <- bank_new[-trainIndex,]

featurePlot(x = bank_new[-1],
            y = bank_new$y,
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

## Project Bank - Feature Plot

```{r echo=TRUE, eval = FALSE}
featurePlot(x = bank_new[-1],
            y = bank_new$y,
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

## Project Bank - Train

```{r echo=TRUE, eval = FALSE}
train_control <- trainControl(
    method = 'repeatedcv',           # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = TRUE,               # should class probabilities be returned
)

if (FALSE) {
  # Running time is too long. Skip running.
  adaboost_fit <- train(y ~ .,
                    data = bank_train,
                   method='adaboost',
                   tuneLength=2,
                   trControl = train_control)
  adaboost_fit
  
  predictions <- predict(adaboost_fit, newdata = bank_train)
  confusionMatrix(predictions, bank_train$y)
  
  predictions <- predict(adaboost_fit, bank_test)
  confusionMatrix(predictions, bank_test$y)
}

# Logistic regression
log_fit <- train(y ~ .,
                 data = bank_train,
                 method = "glm",
                 family = binomial,
                 trControl = train_control)

predictions <- predict(log_fit, newdata = bank_train)
confusionMatrix(predictions, bank_train$y)

predictions <- predict(log_fit, bank_test)
confusionMatrix(predictions, bank_test$y)
# which(test$y != predictions)
```

## Project Bank - Train with Decision Tree

```{r echo=TRUE, eval = FALSE}
# Recursive Partitioning and Regression Trees
rpart_fit <- train(y ~ .,
                   data = bank_train,
                   method="rpart",
                   trControl = train_control)
predictions <- predict(rpart_fit, bank_train)
confusionMatrix(predictions, bank_train$y)

predictions <- predict(rpart_fit, bank_test)
confusionMatrix(predictions, bank_test$y)
```

```{r echo=TRUE, eval = FALSE}
rattle::ggVarImp(rpart_fit$finalModel, log=TRUE)
rattle::fancyRpartPlot(rpart_fit$finalModel)
```

## Project Bank - Model comparison

```{r echo=TRUE, eval = FALSE}
models_compare <- resamples(list(RP = rpart_fit, GLM = log_fit))
                            
# Summary of the models performances
summary(models_compare)
```

## Project Australia Weather

- A complete project with some feature engineering.

<https://www.dropbox.com/s/p73mdxcrx05mbwb/aus_weather_predict.Rmd?dl=1>